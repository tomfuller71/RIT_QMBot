{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Add the parent directory to sys.path so Python can find the Utils module\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from Utils import (get_guidance_and_feedback, get_questions_and_answers, print_bot_response, create_content)\n",
    "\n",
    "# Load the API key from the .env file\n",
    "config = dotenv_values('../.env')\n",
    "client = OpenAI(api_key=config['OPENAI_API_KEY'])\n",
    "tutor_agent_id = config['TUTOR_AGENT_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the content for our prompt message to our bot.  Our prompt must look like the input examples we provided to our assistant in its instructions.  For this toy example we are using JSON files of question data and student answers as a store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_path = Path('Data/question_store.json')\n",
    "a_path = Path('Data/student_answers.json')\n",
    "questions, answers = get_questions_and_answers(q_path, a_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded the question and answer data we can now create input content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_id = \"Q2\"\n",
    "student_id = \"student_2\"\n",
    "\n",
    "question = questions[question_id]\n",
    "student_answer = answers[question_id][student_id]\n",
    "\n",
    "content = create_content(question, student_answer)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start a new conversation thread and send our content as a messages to our assistant.  This can take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread, messages = get_guidance_and_feedback(\n",
    "    client, content, tutor_agent_id, thread_id=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the messages we can print out the bot's text response which is embedded within the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = print_bot_response(client, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have started the thread, we can continue if we want the chat with the bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up = \"I am not sure about the answer. Can you provide more guidance?\"\n",
    "\n",
    "thread, messages = get_guidance_and_feedback(\n",
    "    client, follow_up, tutor_agent_id, thread_id=thread\n",
    ")\n",
    "\n",
    "print_bot_response(client, messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
